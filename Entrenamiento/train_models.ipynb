{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv(\"../input/text-data/dataset.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = list(df[\"Text\"])\nprint(type(sentences))\nprint(len(sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_filter = []\nfor row in sentences:\n    row = row.lower()\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    new_words = tokenizer.tokenize(row)\n    new_row = [word for word in new_words  if word not in stop_words]\n    new_sentence = \" \".join(new_row)\n    sentences_filter.append(new_sentence)\n\nprint(sentences[0][:100])\nprint(\"Converted:\")\nprint(sentences_filter[0][:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(sentences_filter)\nword_index = tokenizer.word_index\nprint(word_index)\ntotal_words = list(word_index.keys())\nprint(len(total_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import ngrams, FreqDist\nfull_text = []\nfor row in sentences_filter:\n    full_text.extend(row.split(\" \"))\nn_gramas = ngrams(total_words, n=1)\nfreq = FreqDist(full_text)\nfreq.plot(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_list = [] #filtramos tokens que se repiten mas de una vez\nfor key,val in freq.items():\n    if (val==1):\n        check_list.append(str(key))\n        \nprint(check_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sentences = []\nother_stop_w = [\"si\", \"usted\", \"señor\", \"gracias\", \"señorita\"]\ncheck_list.extend(other_stop_w)\nfor row in sentences_filter:\n    words = row.split(\" \")\n    final_words = []\n    for w in words:\n        if w not in check_list:\n            final_words.append(w)\n    final_sentences.append(\" \".join(final_words))\n    \nprint(sentences_filter[0][:200])\nprint(final_sentences[0][:200])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(final_sentences)\nword_index = tokenizer.word_index\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_words = list(word_index.keys())\nprint(len(total_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(final_sentences)\nsequences_np = np.array(sequences)\nmylen = np.vectorize(len)\nseq_sizes = mylen(sequences_np)\nprint(\"Min sequence length:\", np.min(seq_sizes))\nprint(\"Max sequence length:\", np.max(seq_sizes))\nprint(\"Avg sequence length:\", np.mean(seq_sizes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nembedding_dim = 300\nmax_length = 320\ntrunc_type='post'\n\npadded = pad_sequences(sequences,maxlen=max_length, padding='post', truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"text_file = \"../input/pretrained-word-vectors-for-spanish/SBW-vectors-300-min5.txt\"\n\n\ndata = []\nwith open(text_file, 'r') as fopen:\n    for n, line in enumerate(fopen):\n        data.append(line)\n        if (n + 1) % 100000 == 0:\n            print('done processed: ', n + 1)\n            \nprint('done!')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = total_words\nimport numpy\ndict_words_vect = {}\nfor row in data:\n        row = row[:-1]\n        word = row.split(\" \")[0]\n        if word in vocab:\n            vector = row.split(\" \")[1:]\n            vector = np.array([float(x) for x in vector])\n            dict_words_vect[word] = vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(vocab))\nprint(len(dict_words_vect))\nprint(dict_words_vect[\"tarjeta\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_in_dict = list(dict_words_vect.keys())\nmissing_words = list(set(vocab) - set(words_in_dict))\nprint(missing_words)\nfor word in missing_words:\n    dict_words_vect[word] = np.zeros(300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dict_words_vect[\"netflix\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 300\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = dict_words_vect[word];\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embeddings_matrix)\nprint(len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_words = list(word_index.keys())\nprint(list_words[0])\nprint(list_words[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_index[\"<OOV>\"])\nprint(word_index[\"transfieren\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dict_words_vect[\"<OOV>\"][:10])\nprint(dict_words_vect[\"transfieren\"][:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embeddings_matrix[0][:10])\nprint(embeddings_matrix[-1][:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_producto = list(df[\"Producto\"])\nprint(len(labels_producto))\nn_classes = len(set(labels_producto))\nprint(n_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(labels_producto)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_to_label_prod = {}\nlabel_to_index_prod = {}\nfor i, label in enumerate(set(labels_producto)):\n    index_to_label_prod[i] = label\n    label_to_index_prod[label] = i\n    \nprint(index_to_label_prod)\nprint(label_to_index_prod)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_producto_num = np.array([label_to_index_prod[label] for label in labels_producto])\nprint(labels_producto_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_producto_num[:split]\nvalid_y = labels_producto_num[split:]\nprint(len(train_x))\nprint(len(train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_prod = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(32, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(32), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_prod.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_prod.summary()\n\nnum_epochs = 38\n\nhistory = model_prod.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\ndef plot_h(history):\n\n    #-----------------------------------------------------------\n    # Retrieve a list of list results on training and test data\n    # sets for each training epoch\n    #-----------------------------------------------------------\n    acc=history.history['accuracy']\n    val_acc=history.history['val_accuracy']\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n\n    epochs=range(len(acc)) # Get number of epochs\n\n    #------------------------------------------------\n    # Plot training and validation accuracy per epoch\n    #------------------------------------------------\n    plt.plot(epochs, acc, 'r')\n    plt.plot(epochs, val_acc, 'b')\n    plt.title('Training and validation accuracy')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\n    plt.figure()\n\n    #------------------------------------------------\n    # Plot training and validation loss per epoch\n    #------------------------------------------------\n    plt.plot(epochs, loss, 'r')\n    plt.plot(epochs, val_loss, 'b')\n    plt.title('Training and validation loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Loss\", \"Validation Loss\"])\n\n    plt.figure()\n\n\n    # Expected Output\n    # A chart where the validation loss does not increase sharply!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_stop_words = list(stop_words)\nall_stop_words.extend(check_list)\n\ndef preprocess_text(input_text):\n    input_text = input_text.lower()\n    nltk_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    new_words = nltk_tokenizer.tokenize(input_text)\n    new_row = [word for word in new_words  if word not in all_stop_words]\n    \n    final_sentence = \" \".join(new_row)\n    sequence = tokenizer.texts_to_sequences([final_sentence])\n    \n    max_length = 320\n    trunc_type='post'\n    padding_type='post'\n\n    padded = pad_sequences(sequence,maxlen=max_length, padding=padding_type, truncating=trunc_type)\n    return padded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_text(input_text, input_model, label_map):\n    padded = preprocess_text(input_text)\n    result = input_model.predict(padded)\n    #print(result)\n    result_index = np.argmax(result)\n    predicted_class = label_map[result_index]\n    \n    return predicted_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_text = \"Hola, soy asesor de línea de bebé A Tengo el gusto. Sí, mucho gusto, señorita. Agradecemos su paciencia debido a la contingencia. El número de asesores disponibles reducido. En qué puedo ayudarla? Si mire, lo que pasa es que hace ratito marque. Me dijeron que regresaría la llamada más o menos entre tres y siete de la noche. Por qué? Porque el día de ayer que la reposición de mi tarjeta ese es de la S o la de débito se ahorra. Entonces es que tenía yo en la tarjeta el día de ciento cuarenta y cuatro pesos retenidos hemos sido ocupado para hacer lo del pago porque lo que dicen que en cuanto se hacen depósitos de descuenta lo del pago de la tarjeta correcto. El día de hoy me hicieron donde Pues tío de seiscientos treinta y siete supone que la tarjeta el plástico costaba ciento ochenta. Entonces treinta y siete del depósito veo y los ciento cuarenta y cuatro que tenía que ya con eso completaba hoy Marco para checar mi saldo y me dicen que tengo cuatrocientos noventa y dos. Se supone que tenía que haber quedado seiscientos pesos y quería ver qué es lo que pasó muy bien. Le comento No se activó la tarjeta. El día de hoy me aparece la activación. Bueno, la activación aparece hasta el día de hoy, nueve de septiembre del dos mil veinte. Bueno, ya me entregaron el plato y me dijeron que fuera el casero hacerlo de la activación. Correcto. Muy bien. El día de hoy recibe un abono por seiscientos treinta y siete pesos. Correcto. La tarjeta literalmente estaba en ceros. El día de hoy recibe una buena por seiscientos treinta y siete pesos. Pero si alguien revisa la tarjeta y estaba en silencio ahí, en ciento cuarenta y cuatro, pero estaba el número negativo, es decir, al momento de activarla, se genera el pago, el cobro y el cobro se genera, pero en saldo negativo. Es decir, usted activa la tarjeta y en ese mismo instante, sistema le carga el monto de ciento cuarenta y cuatro presos con sesenta y tres centavos. Saldo negativo que se estaría cobrando el cobro de la tarjeta, que son ciento cuarenta y cinco pesos. Se manejan los ciento cuarenta y cuatro pesos con sesenta y tres centavos aquí. Al parecer, usted tenía un saldo disponible de cero seis centavos. Entonces el cero seis centavos se cobran estos ciento cuarenta y cuatro pesos, con sesenta y tres centavos se quedan en saldo negativo. Porque porque la tarjeta está literalmente en cero. Al momento de que usted le deposita la cantidad de seiscientos treinta y siete pesos, se toman los ciento cuarenta y cuatro punto sesenta y tres para cubrir el cargo del la reposición de tarjeta y usted se queda un saldo con cuatrocientos, noventa y dos pesos con treinta y siete centavos. Muy probablemente usted pensó que tenía los ciento cuarenta y cuatro pesos con sesenta y tres centavos a favor, pero no sé, ya los tenía en saldo negativo porque su cuenta estaba en cero. Es lo que yo tenía duda. Pues sí, era lo que yo tenía ahí de saldo de que ya tiene tiempo de la tarjeta o eran por lo ahora, por lo que se cobraba. Correcto, Es usted. Es decir, usted no tenía ni un solo centavo en bueno, tenía la Navidad de seis centavos en la cuenta, por lo cual pues únicamente le realizan el cargo de cuatro ciento cuarenta y cuatro pesos, con sesenta y tres centavos, ya que el cargo original es de ciento cuarenta y cinco pesos. En este caso se queda el restante. Por los centavos que se toman se queda el mensaje de ciento cuarenta y cuatro pesos, con sesenta y tres centavos de los cuales, como aparecen en saldo negativo, le tuvieron que tuvo que haber aparecido el signo de negativo de menos la pobreza. Saldo negativo. Entonces, al momento de que usted le deposita en la cantidad de seiscientos treinta y siete pesos, pues ese saldo se convierte en positivo, se cobra. Y usted se le descuenta la cantidad de ciento treinta y siete y se queda con el disponible de cuatrocientos noventa y dos punto treinta y siete. Ah, bueno, si es que yo tenía la duda de que bueno, se supone que alguien ciento cuarenta y cuatro. Ciento cuarenta y cuatro. Entonces, qué pasó? Ajá. Pero no está bien si nomas quería quitarme la verdad correcto, señorita, alguna otra información que puede apoyarla? Tendría que ir a tendría que ir al banco si quisiera que me lleguen las notificaciones de mi celular, del, de la de los depósitos. Muy bien, vamos a validar la información. No tiene usted activada la aplicación? Es que que creo que mi celular no me agarra la aplicación. Muy bien. En este caso, necesitaríamos que para que le lleguen las notificaciones o las alertas inicialmente, tener la aplicación móvil para que a partir de ahí se activó en las alertas y le lleguen todas las notificaciones respecto algún cargo, algún cobro, alguna compra por pensar que normal no llegarían. Podría ser con las alertas, señorita. Pero en este caso tendría que ser y activarlas directamente en sucursal. Bueno, sería todo entonces. Por último, su opinión es muy importante para nosotros. En las próximas horas en veremos a su correo electrónico una breve encuesta para calificar el servicio proporcionado en esta llamada. Sus comentando que ayudan a mejorar la atención en cada contacto. Puedo contar con usted, señorita? Bueno, y para poder a nombre de encuesta. Se lo agradecería bastante. Reitero mi nombre es Ismael. Asesor de línea de de uve A. Pase una excelente tarde. Igualmente. Gracias. Hasta luego.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_prod, index_to_label_prod)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Intención"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_int = list(df[\"Intención\"])\nprint(len(labels_int))\nn_classes = len(set(labels_int))\nprint(n_classes)\nprint(set(labels_int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_to_label_int = {}\nlabel_to_index_int = {}\nfor i, label in enumerate(set(labels_int)):\n    index_to_label_int[i] = label\n    label_to_index_int[label] = i\n    \nprint(index_to_label_int)\nprint(label_to_index_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_int_num = np.array([label_to_index_int[label] for label in labels_int])\nprint(labels_int_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_int_num[:split]\nvalid_y = labels_int_num[split:]\nprint(len(train_x))\nprint(len(train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_int = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(32, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(16), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_int.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_int.summary()\n\nnum_epochs = 30\n\nhistory = model_int.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_int, index_to_label_int)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Movimiento"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_movimiento = list(df[\"Tipo de movimiento\"])\nprint(len(labels_movimiento))\nn_classes = len(set(labels_movimiento))\nprint(n_classes)\n\n\nindex_to_label_mov = {}\nlabel_to_index_mov = {}\nfor i, label in enumerate(set(labels_movimiento)):\n    index_to_label_mov[i] = label\n    label_to_index_mov[label] = i\n    \nprint(index_to_label_mov)\nprint(label_to_index_mov)\n\n\nlabels_movimiento_num = np.array([label_to_index_mov[label] for label in labels_movimiento])\nprint(labels_movimiento_num)\n\n\nsplit = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_movimiento_num[:split]\nvalid_y = labels_movimiento_num[split:]\nprint(len(train_x))\nprint(len(train_y))\n\n\nmodel_mov = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(32), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_mov.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_mov.summary()\n\nnum_epochs = 33\n\nhistory = model_mov.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_mov, index_to_label_mov)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Contexto 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Model_contx1o para contexto 1\n\nlabels_contexto = list(df[\"Contexto 1\"])\nprint(len(labels_contexto))\nn_classes = len(set(labels_contexto))\nprint(n_classes)\n\n\nindex_to_label_contx1 = {}\nlabel_to_index_contx1 = {}\nfor i, label in enumerate(set(labels_contexto)):\n    index_to_label_contx1[i] = label\n    label_to_index_contx1[label] = i\n    \nprint(index_to_label_contx1)\nprint(label_to_index_contx1)\n\n\nlabels_contexto_num = np.array([label_to_index_contx1[label] for label in labels_contexto])\nprint(labels_contexto_num)\n\n\nsplit = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_contexto_num[:split]\nvalid_y = labels_contexto_num[split:]\nprint(len(train_x))\nprint(len(train_y))\n\n\nmodel_contx1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    #tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(16, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(16), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_contx1.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_contx1.summary()\n\nnum_epochs = 40\n\nhistory = model_contx1.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_contx1, index_to_label_contx1)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Contexto 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_context_2 = list(df[\"Contexto 2\"])\nprint(len(labels_context_2))\nn_classes = len(set(labels_context_2))\nprint(n_classes)\n\n\nindex_to_label_contx2 = {}\nlabel_to_index_contx2 = {}\nfor i, label in enumerate(set(labels_context_2)):\n    index_to_label_contx2[i] = label\n    label_to_index_contx2[label] = i\n    \nprint(index_to_label_contx2)\nprint(label_to_index_contx2)\n\n\nlabels_context_2_num = np.array([label_to_index_contx2[label] for label in labels_context_2])\nprint(labels_context_2_num)\n\n\nsplit = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_context_2_num[:split]\nvalid_y = labels_context_2_num[split:]\nprint(len(train_x))\nprint(len(train_y))\n\n\nmodel_contx2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(32), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_contx2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_contx2.summary()\n\nnum_epochs = 32\n\nhistory = model_contx2.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_contx2, index_to_label_contx2)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Detalle 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_detalle = list(df[\"Detalle 1\"])\nprint(len(labels_detalle))\nn_classes = len(set(labels_detalle))\nprint(n_classes)\n\n\nindex_to_label_detalle = {}\nlabel_to_index_detalle = {}\nfor i, label in enumerate(set(labels_detalle)):\n    index_to_label_detalle[i] = label\n    label_to_index_detalle[label] = i\n    \nprint(index_to_label_detalle)\nprint(label_to_index_detalle)\n\n\nlabels_detalle_num = np.array([label_to_index_detalle[label] for label in labels_detalle])\nprint(labels_detalle_num)\n\n\nsplit = int(len(padded)*0.9)\ntrain_x = padded[:split]\nvalid_x = padded[split:]\ntrain_y = labels_detalle_num[:split]\nvalid_y = labels_detalle_num[split:]\nprint(len(train_x))\nprint(len(train_y))\n\n\nmodel_detalle = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'), #64->32\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(32), #64->32\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\nmodel_detalle.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_detalle.summary()\n\nnum_epochs = 32\n\nhistory = model_detalle.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=num_epochs)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_h(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = predict_text(new_text, model_detalle, index_to_label_detalle)\nprint(\"RESULTADO:\")\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# save all"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\n#saving model\nmodel_prod.save('model_producto.h5')\nmodel_int.save('model_intencion.h5')\nmodel_mov.save('model_movimiento.h5')\nmodel_contx1.save('model_contexto_1.h5')\nmodel_contx2.save('model_contexto_2.h5')\nmodel_detalle.save('model_detalle.h5')\n\n\n# saving text tokenizer\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# saving text tokenizer\nwith open('all_stop_words.pickle', 'wb') as handle:\n    pickle.dump(all_stop_words, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# saving label map producto\nwith open('index_to_label_prod.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_prod, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# saving label map intentcion\nwith open('index_to_label_int.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_int, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    \n# saving label map movimiento\nwith open('index_to_label_mov.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_mov, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    \n# saving label map context 2\nwith open('index_to_label_contx1.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_contx1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    \n# saving label map context 2\nwith open('index_to_label_contx2.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_contx2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# saving label map detalle\nwith open('index_to_label_detalle.pickle', 'wb') as handle:\n    pickle.dump(index_to_label_detalle, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r export.zip .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}